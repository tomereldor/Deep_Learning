{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network from Scratch\n",
    "### Tomer Eldor\n",
    "#### TT Deep Learning 1 \n",
    "#### Oct 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomer/miniconda3/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/tomer/miniconda3/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Data\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata # for importing the dataset only\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = fetch_mldata('MNIST original')\n",
    "X_full, y_full = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESS DATA ###\n",
    "n_classes = 10\n",
    "\n",
    "# Reduce to a susbsample (while working on the code)\n",
    "SUBSAMPLE_SIZE = 50000 # choose how many samples for now\n",
    "subsample_idx = np.random.choice(X_full.shape[0], SUBSAMPLE_SIZE) # array of indices of subsample\n",
    "X, y = X_full[subsample_idx], y_full[subsample_idx]\n",
    "\n",
    "# Shuffle X and Y for random sampling\n",
    "index_random = np.random.permutation(X.shape[0])\n",
    "X, y = X[index_random], y[index_random]\n",
    "n_train_samples = int(0.75*SUBSAMPLE_SIZE)\n",
    "\n",
    "\n",
    "# Preprocess inputs and outputs\n",
    "def preprocess_bw_image(x):\n",
    "    '''Normalizes pixel values to be between 0 and 1 instead of 0â€“255'''\n",
    "    return x/255.0\n",
    "X_processed = preprocess_bw_image(X)\n",
    "\n",
    "def onehot(y, n_classes):\n",
    "    '''Reshapes y to a one-hot coding and flips the matrix shape to fit later calculations'''\n",
    "    return np.eye(n_classes)[y.astype('int')] \n",
    "y_onehot = onehot(y, n_classes) # convert y to one-hot encoding, to compare with y_pred (for loss function)\n",
    "\n",
    "# Divide into Training and Test Samples and Flip matrices for later computations\n",
    "X_train, X_test = X_processed[:n_train_samples].T, X_processed[n_train_samples:].T\n",
    "Y_train, Y_test = y_onehot[:n_train_samples].T, y_onehot[n_train_samples:].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NEURAL NETWORK MLP CLASS\n",
    "\n",
    "class neural_net_mlp:\n",
    "    \n",
    "    def __init__(self, X, Y, n_hidden_nodes = 15, n_classes = 10, learning_rate = 0.5):\n",
    "        ''' \n",
    "        Initializaing MLP Neural Network with one input layer, one hidden layer, and output layer.\n",
    "        Arguments:\n",
    "            X - inputs (training). shape: (N_features, N_samples) (*features = pixels here)\n",
    "            Y - output labels of X_training, one-hot encoded. shape (N_classes, N_samples)\n",
    "            n_hidden_nodes - number of neurons in the hidden layer \n",
    "            n_classes - number of possible classes for Y\n",
    "            learning_rate - rate at which the network will train (in gradient descent)\n",
    "        '''\n",
    "        # initializing self NN parameters\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.n_hidden_nodes = n_hidden_nodes\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_X_nodes = X.shape[0] # length of features/pixels of each observation\n",
    "        # Initialize Weights and Biases\n",
    "        self.Weights1 = np.random.normal(0, 1, size=(self.n_hidden_nodes, self.n_X_nodes))\n",
    "        self.biases1 = np.random.normal(0, 1, size=(self.n_hidden_nodes, 1))\n",
    "        self.Weights2 = np.random.normal(0, 1, size=(self.n_classes, self.n_hidden_nodes))\n",
    "        self.biases2 = np.random.normal(0, 1, size=(self.n_classes, 1))\n",
    "    \n",
    "    \n",
    "    ## Helper Math Functions\n",
    "    \n",
    "    def loss_function(self, y_true, y_pred):\n",
    "        ''' \n",
    "        Using multiclass cross entropy as a loss function\n",
    "        (since we have multiclass output)\n",
    "        Input: y_true, y_pred - vectors of same shape\n",
    "        Returns: result of multiclass cross entropy between them (float)\n",
    "        '''\n",
    "        loss = np.sum(np.multiply(y_true, np.log(y_pred)))\n",
    "        return -(1/y_true.shape[0]) * loss\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        ''' simple sigmoid function as our activation function of Layer 0 and 1'''\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        ''' derivative of the sigmoid function for backpropagation'''\n",
    "        sig = self.sigmoid(Z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        ''' Softmax activation function. Applies softmax to the input x.\n",
    "        (Its advantage is that it returns a vector of probabilities that sum up to 1)\n",
    "        Arguments:\n",
    "            - x: any vector\n",
    "        Returns:\n",
    "            - softmax(x) - vector of the same shape'''\n",
    "        exps_x = np.exp(x)\n",
    "        return exps_x / np.sum(exps_x, axis=0) \n",
    "\n",
    "    \n",
    "    ## Nerual Network functions\n",
    "    \n",
    "    def fc_layer(self, inputs, weights, biases):\n",
    "        '''\n",
    "        Fully Connected Layer of MLP/NN: y = wx + b (weights*input + biases), \n",
    "        that fed into the activation function, so that y = activation(wx+b)\n",
    "        Arguments:\n",
    "             - inputs: input data for the layer - either the initial input for the net\n",
    "                        or the ouput of the previous (lower) layer (np array of floats)\n",
    "             - weights: set of weights for that layer's connections (np array of floats)\n",
    "             - biases: set of biases for that layer's connections (np array of floats)\n",
    "        Returns:\n",
    "            Z: the result of the matrix multiplication of weights with previous layer's activations\n",
    "        '''\n",
    "        return np.matmul(weights, inputs) + biases\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        '''\n",
    "        Constructs the NNet with 1 hidden layer and 1 output layer\n",
    "        Feeds forward from the initial (preprocessed) input through those layers, generates prediction\n",
    "        Arguments:\n",
    "            - X: the input data for the network (numpy array of floats). size: (n_pixles (or features), n_observations)\n",
    "            - Weights1 - initial values for weights of the first layer. np array of size: (n_hidden_nodes, n_initial_input_nodes)\n",
    "            - biases1 - initial values for biases. vector of length: n_hidden_nodes\n",
    "            - Weights2 - initial values for weights of the first layer. np array of size: (n_output_nodes, n_hidden_nodes)\n",
    "            - biases2 - initial values for biases. vector of length: n_output_nodes\n",
    "        Returns:\n",
    "            - y_pred - prediction probability for each observation for each class\n",
    "            - Z1: result of the fully connecter layer 1 (y=wx+b, before activation), needed for later gradient/derivatives\n",
    "            - A1: result of Z1 after the activation function, needed for backpropagation later\n",
    "        '''\n",
    "        # FEED-FORWARD \n",
    "        # layer 0 - input layer\n",
    "        self.A0 = X # allowing flexible input for prediction\n",
    "        # layer 1 - hidden layer\n",
    "        Z1 = self.fc_layer(self.A0, self.Weights1, self.biases1)\n",
    "        A1 = self.sigmoid(Z1)\n",
    "        # layer 2 - output layer\n",
    "        Z2 = self.fc_layer(A1, self.Weights2, self.biases2)\n",
    "        # for the last layer, we'll yse softmax as the activation function \n",
    "        # to output probabilities summing up to 1 (for above 2 classes)\n",
    "        y_pred = self.softmax(Z2)\n",
    "        return y_pred, Z1, A1 # Z1 & A1 needed for backpropagation\n",
    "\n",
    "    def gradients_WB(self, dZ, x):\n",
    "        ''' \n",
    "        Calculates the derivatives with respect to the weights and biases.\n",
    "        See https://www.deeplearningbook.org/contents/mlp.html, page 209\n",
    "        Arguments:\n",
    "            dZ: derivative with respect to Z of that layer (depends on the activation)\n",
    "            X: training examples used\n",
    "            n_train_samples = number of training \n",
    "        Returns:\n",
    "            Gradient for weights, gradient for biases\n",
    "        '''\n",
    "        n_obs = self.X.shape[0]\n",
    "        dWeights = (1.0/n_obs) * np.matmul(dZ, x.T)\n",
    "        dBiases = (1.0/n_obs) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        return dWeights, dBiases\n",
    "\n",
    "\n",
    "    ## TRAIN THE NETWORK \n",
    "    \n",
    "    def train(self, n_epochs = 100, n_prints = 10, showprint = True, showplot = True):\n",
    "        ''' Train the NNet\n",
    "        Arguments:\n",
    "            - n_epochs: number of repetitions for the training process (int)\n",
    "            - n_prints: number of epochs to save and plot or print their cost function (int)\n",
    "            - showprint: print cost function lines or not (Boolean)\n",
    "            - showplot: display plot of loss function over time or not (Boolean)\n",
    "        Returns:\n",
    "            final Cost value from loss function (float)\n",
    "            Trains the network internally (self.weights and biases), outputs prints and plots if desired\n",
    "        '''\n",
    "        # initialize and save training parameters info\n",
    "        self.costs = []\n",
    "        self.n_epochs = n_epochs \n",
    "        epoch_ids = []\n",
    "        # TRAIN\n",
    "        if showprint: print(\"Training MLP Neural Network with 1 hidden layer of {} neurons for {} epochs\".format(self.n_hidden_nodes, self.n_epochs))\n",
    "        for i in range(1, n_epochs+1):\n",
    "            # FEED-FORWARD \n",
    "            y_pred, Z1, A1 = self.feed_forward(self.X)\n",
    "            # calculate cost using the loss function\n",
    "            cost = self.loss_function(self.Y, y_pred)\n",
    "\n",
    "            # BACKPROPAGATION - gradient calculations\n",
    "            # derivatives of Layer 2: \n",
    "            dZ2 = y_pred - self.Y # simple error\n",
    "            dWeights2, dBiases2 = self.gradients_WB(dZ2, A1)\n",
    "            # derivatives of Layer 1\n",
    "            dA1 = np.matmul(self.Weights2.T, dZ2) # derivative of A1 \n",
    "            dZ1 = dA1 * self.sigmoid_derivative(Z1) # derivative of Z1: derivative of A1 and sigmoid derivative of Z1\n",
    "            dWeights1, dBiases1 = self.gradients_WB(dZ1, self.X)\n",
    "\n",
    "            # Update the weights and biases\n",
    "            self.Weights2 -= self.learning_rate * dWeights2\n",
    "            self.biases2 -=  self.learning_rate * dBiases2\n",
    "            self.Weights1 -= self.learning_rate * dWeights1\n",
    "            self.biases1 -=  self.learning_rate * dBiases1\n",
    "\n",
    "            # (print) and save cost results\n",
    "            if i == 1 or (i % int(self.n_epochs / n_prints) == 0) or i == n_epochs:\n",
    "                if showprint: print(\"Cost at Epoch {}: \\t {}\".format(i, cost))\n",
    "                self.costs.append(cost)\n",
    "                epoch_ids.append(i)\n",
    "        \n",
    "        # Summarize and plot costs\n",
    "        if showplot:\n",
    "            plt.plot(epoch_ids, self.costs)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss (Multiclass Cross-Entropy)')\n",
    "            plt.title('NN Loss function during training epochs')\n",
    "            plt.show()\n",
    "        \n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP Neural Network with 1 hidden layer of 15 neurons for 5000 epochs\n",
      "Cost at Epoch 1: \t 3599.0708866008354\n",
      "Cost at Epoch 250: \t 245.81861380031089\n",
      "Cost at Epoch 500: \t 168.18777157251407\n",
      "Cost at Epoch 750: \t 131.2072820412647\n",
      "Cost at Epoch 1000: \t 108.71133635316615\n",
      "Cost at Epoch 1250: \t 93.41058010598938\n",
      "Cost at Epoch 1500: \t 81.79485595174926\n",
      "Cost at Epoch 1750: \t 72.72274793832061\n",
      "Cost at Epoch 2000: \t 65.5158967334727\n",
      "Cost at Epoch 2250: \t 59.46187949431132\n",
      "Cost at Epoch 2500: \t 54.74235842770605\n",
      "Cost at Epoch 2750: \t 50.46233552636417\n",
      "Cost at Epoch 3000: \t 46.75588986241054\n",
      "Cost at Epoch 3250: \t 44.08113507929413\n",
      "Cost at Epoch 3500: \t 41.619689589631825\n",
      "Cost at Epoch 3750: \t 39.45992866362695\n",
      "Cost at Epoch 4000: \t 37.378319040566105\n",
      "Cost at Epoch 4250: \t 35.191362257890965\n",
      "Cost at Epoch 4500: \t 33.76120534615713\n",
      "Cost at Epoch 4750: \t 32.32027328313764\n",
      "Cost at Epoch 5000: \t 31.048684652495055\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a Neural Network (MLP) object\n",
    "nn = neural_net_mlp(X_train, Y_train, n_hidden_nodes = 15, n_classes = 10, learning_rate = 0.5)\n",
    "nn.train(n_epochs = 5000, n_prints = 20, showprint = 1, showplot = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set:  123.02190720993393\n"
     ]
    }
   ],
   "source": [
    "y_pred, _Z1, _A1 = nn.feed_forward(X_test)\n",
    "cost = nn.loss_function(Y_test, y_pred)\n",
    "print(\"Loss of test set: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show results with visual predictions\n",
    "def print_image_and_prediction(i, samples, X_test, y_pred):\n",
    "    example = X_test[:, samples[i]].reshape(28,28)\n",
    "    plt.imshow(example, cmap = plt.cm.gray)\n",
    "    predicted_result = np.argmax(y_pred[:,samples[i]])\n",
    "    plt.title(\"Predicted: {}\".format(predicted_result))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "samples = np.random.randint(X_test.shape[1], size = 10) #draw some random samples to plot\n",
    "for i in range(1, 9):\n",
    "    ax = fig.add_subplot(2, 4, i) # 2 rows with 4 images (cols)\n",
    "    print_image_and_prediction(i, samples, X_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The model seems to have trained successfully, as seen by the cost being reduced as expected (steep decrease initially, with later a less steep slope). \n",
    "The cost of the same model for the test set was higher than the final cost of the training set, so we might have overfitted. \n",
    "I have tried to increase the subsample size from 10K to 50K, train it again for less epochs searching for a balance cost between train and test samples, varying the number of nodes in the hidden layer and the learning rate.\n",
    "Finding the best combination of those would be ideal and can be done using a grid search through many values of each. Below is a smaller test searching through multiple values of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Calibrating\n",
    "*This I continued on my own to explore better parameters *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training an MLP with 15 hidden nodes and learning rate of 0.1, for 500 epochs\n",
      "Training loss: 1139.3299834088782, Test loss: 431.3432909762559 - for MLP with 15 hidden nodes, learning rate = 0.1, trained for 500 epochs\n",
      "Training an MLP with 15 hidden nodes and learning rate of 0.1, for 1000 epochs\n",
      "Training loss: 962.4741105713491, Test loss: 387.89927148871357 - for MLP with 15 hidden nodes, learning rate = 0.1, trained for 1000 epochs\n",
      "Training an MLP with 15 hidden nodes and learning rate of 0.1, for 5000 epochs\n",
      "Training loss: 464.48229967370656, Test loss: 323.447861528466 - for MLP with 15 hidden nodes, learning rate = 0.1, trained for 5000 epochs\n",
      "Training an MLP with 15 hidden nodes and learning rate of 1, for 500 epochs\n",
      "Training loss: 23589.764502253624, Test loss: 6102.375646366163 - for MLP with 15 hidden nodes, learning rate = 1, trained for 500 epochs\n",
      "Training an MLP with 15 hidden nodes and learning rate of 1, for 1000 epochs\n",
      "Training loss: 25873.04401913751, Test loss: 7820.972817084575 - for MLP with 15 hidden nodes, learning rate = 1, trained for 1000 epochs\n",
      "Training an MLP with 15 hidden nodes and learning rate of 1, for 5000 epochs\n",
      "Training loss: 232222.39761898282, Test loss: 68933.56709475904 - for MLP with 15 hidden nodes, learning rate = 1, trained for 5000 epochs\n",
      "Training an MLP with 15 hidden nodes and learning rate of 10, for 500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomer/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/tomer/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in multiply\n",
      "/Users/tomer/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:58: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: nan, Test loss: nan - for MLP with 15 hidden nodes, learning rate = 10, trained for 500 epochs\n",
      "Training an MLP with 15 hidden nodes and learning rate of 10, for 1000 epochs\n",
      "Training loss: nan, Test loss: nan - for MLP with 15 hidden nodes, learning rate = 10, trained for 1000 epochs\n",
      "Training an MLP with 15 hidden nodes and learning rate of 10, for 5000 epochs\n",
      "Training loss: nan, Test loss: nan - for MLP with 15 hidden nodes, learning rate = 10, trained for 5000 epochs\n",
      "Training an MLP with 64 hidden nodes and learning rate of 0.1, for 500 epochs\n",
      "Training loss: 737.2620313710697, Test loss: 315.6828725082976 - for MLP with 64 hidden nodes, learning rate = 0.1, trained for 500 epochs\n",
      "Training an MLP with 64 hidden nodes and learning rate of 0.1, for 1000 epochs\n",
      "Training loss: 488.99439009616566, Test loss: 271.09940639612523 - for MLP with 64 hidden nodes, learning rate = 0.1, trained for 1000 epochs\n",
      "Training an MLP with 64 hidden nodes and learning rate of 0.1, for 5000 epochs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-0cad83023575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training an MLP with {} hidden nodes and learning rate of {}, for {} epochs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_net_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mtraining_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_prints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Z1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_A1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtest_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-158031b723c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, n_prints, showprint, showplot)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mdA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeights2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# derivative of A1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# derivative of Z1: derivative of A1 and sigmoid derivative of Z1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mdWeights1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdBiases1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients_WB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Update the weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-158031b723c3>\u001b[0m in \u001b[0;36mgradients_WB\u001b[0;34m(self, dZ, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m         '''\n\u001b[1;32m    116\u001b[0m         \u001b[0mn_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mdBiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdBiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_results = []\n",
    "for hiddens in [15,64,100]:\n",
    "    for alpha in [0.1, 1, 10]:\n",
    "        for n_epochs in [1000,5000]:\n",
    "            print(\"Training an MLP with {} hidden nodes and learning rate of {}, for {} epochs\".format(hiddens, alpha, n_epochs))\n",
    "            nn = neural_net_mlp(X_train, Y_train, n_hidden_nodes = hiddens, n_classes = 10, learning_rate = alpha)\n",
    "            training_cost = nn.train(n_epochs = n_epochs, n_prints = 20, showprint = 0, showplot = 0)\n",
    "            y_pred, _Z1, _A1 = nn.feed_forward(X_test)\n",
    "            test_cost = nn.loss_function(Y_test, y_pred)\n",
    "            print(\"Training loss: {}, Test loss: {} - for MLP with {} hidden nodes, learning rate = {}, trained for {} epochs\".format(training_cost, test_cost, hiddens, alpha, n_epochs))\n",
    "            model_results.append([hiddens, alpha, n_epochs, training_cost, test_cost])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ \n",
    "\n",
    "*I Stopped the above process in the middle to improve parameters again\n",
    "The above process showed that loss was best the smaller the learning rate was (0.1) and the more epochs.\n",
    "That makes sense because the smaller the learning rate, the more sensitive the network is to find the best direction in the gradient to descend to and doesn't 'jump' over the local minima, so the cost should go down more smoothly and consistently, still if more slowly. Therefore, the more iterations (epochs) we have, the more time the NN has to callibrate its weights and biases and descend the gradient, to decrease the cost.\n",
    "\n",
    "Therefore, I'll explore more around that space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with 15 hidden nodes and 0.01 learning rate, for 1000 epochs\n",
      "\t Training loss: 2100.2401974618147, Test loss: 716.3888629346984 - \n",
      "\t MLP with 15 hidden nodes, learning rate = 0.01, 1000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 15 hidden nodes and 0.01 learning rate, for 5000 epochs\n",
      "\t Training loss: 1124.557277104484, Test loss: 431.56013769511776 - \n",
      "\t MLP with 15 hidden nodes, learning rate = 0.01, 5000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 15 hidden nodes and 0.1 learning rate, for 1000 epochs\n",
      "\t Training loss: 888.2159500833876, Test loss: 372.96537013681314 - \n",
      "\t MLP with 15 hidden nodes, learning rate = 0.1, 1000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 15 hidden nodes and 0.1 learning rate, for 5000 epochs\n",
      "\t Training loss: 396.4551560335651, Test loss: 288.79037963528236 - \n",
      "\t MLP with 15 hidden nodes, learning rate = 0.1, 5000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 15 hidden nodes and 0.5 learning rate, for 1000 epochs\n",
      "\t Training loss: 1764.435705561902, Test loss: 684.5704487811058 - \n",
      "\t MLP with 15 hidden nodes, learning rate = 0.5, 1000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 15 hidden nodes and 0.5 learning rate, for 5000 epochs\n",
      "\t Training loss: 726.7681813586308, Test loss: 430.9250154578911 - \n",
      "\t MLP with 15 hidden nodes, learning rate = 0.5, 5000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 64 hidden nodes and 0.01 learning rate, for 1000 epochs\n",
      "\t Training loss: 1488.601434186049, Test loss: 540.8021522490143 - \n",
      "\t MLP with 64 hidden nodes, learning rate = 0.01, 1000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 64 hidden nodes and 0.01 learning rate, for 5000 epochs\n",
      "\t Training loss: 726.3003994647091, Test loss: 325.3646720028156 - \n",
      "\t MLP with 64 hidden nodes, learning rate = 0.01, 5000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 64 hidden nodes and 0.1 learning rate, for 1000 epochs\n",
      "\t Training loss: 488.5563655192794, Test loss: 266.8533588770629 - \n",
      "\t MLP with 64 hidden nodes, learning rate = 0.1, 1000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 64 hidden nodes and 0.1 learning rate, for 5000 epochs\n",
      "\t Training loss: 87.60789859398511, Test loss: 209.7114567876196 - \n",
      "\t MLP with 64 hidden nodes, learning rate = 0.1, 5000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 64 hidden nodes and 0.5 learning rate, for 1000 epochs\n",
      "\t Training loss: 919.9804297096879, Test loss: 425.6251111617231 - \n",
      "\t MLP with 64 hidden nodes, learning rate = 0.5, 1000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 64 hidden nodes and 0.5 learning rate, for 5000 epochs\n",
      "\t Training loss: 502.19549271278015, Test loss: 356.13618718819714 - \n",
      "\t MLP with 64 hidden nodes, learning rate = 0.5, 5000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 100 hidden nodes and 0.01 learning rate, for 1000 epochs\n",
      "\t Training loss: 1336.8607908483657, Test loss: 518.7837227048149 - \n",
      "\t MLP with 100 hidden nodes, learning rate = 0.01, 1000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 100 hidden nodes and 0.01 learning rate, for 5000 epochs\n",
      "\t Training loss: 574.7920114750262, Test loss: 288.58150181011666 - \n",
      "\t MLP with 100 hidden nodes, learning rate = 0.01, 5000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 100 hidden nodes and 0.1 learning rate, for 1000 epochs\n",
      "\t Training loss: 389.9886995405208, Test loss: 228.60809980569934 - \n",
      "\t MLP with 100 hidden nodes, learning rate = 0.1, 1000 epochs \n",
      "\n",
      "\n",
      "Training MLP with 100 hidden nodes and 0.1 learning rate, for 5000 epochs\n"
     ]
    }
   ],
   "source": [
    "# Storing results\n",
    "model_results = []\n",
    "y_preds_first_hundred = []\n",
    "for hiddens in [15,64,100]:\n",
    "    for alpha in [0.01, 0.1, 0.5]:\n",
    "        for n_epochs in [1000,5000]:\n",
    "            print(\"Training MLP with {} hidden nodes and {} learning rate, for {} epochs\".format(hiddens, alpha, n_epochs))\n",
    "            nn = neural_net_mlp(X_train, Y_train, n_hidden_nodes = hiddens, n_classes = 10, learning_rate = alpha)\n",
    "            training_cost = nn.train(n_epochs = n_epochs, n_prints = 20, showprint = 0, showplot = 0)\n",
    "            y_pred, _Z1, _A1 = nn.feed_forward(X_test)\n",
    "            test_cost = nn.loss_function(Y_test, y_pred)\n",
    "            print(\"\\t Training loss: {}, Test loss: {} - \\n\\t MLP with {} hidden nodes, learning rate = {}, {} epochs \\n\\n\".format(training_cost, test_cost, hiddens, alpha, n_epochs))\n",
    "            model_results.append([hiddens, alpha, n_epochs, training_cost, test_cost])\n",
    "            y_preds_first_hundred.append(y_pred[:,:100])\n",
    "            \n",
    "## Stopped this in the middle to optimize parameters again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
