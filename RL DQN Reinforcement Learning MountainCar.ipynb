{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning (DQN) on MountainCar_V0 Environment\n",
    "=====================================\n",
    "\n",
    "##### Please view this page live on GitHub at: https://gist.github.com/tomereldor/255576a756c27fec4c046a8550ea9562 \n",
    "This implementation is based on Pytorch's official tutorial for reinforcement learning DQN algorithm, with the Cartpole environment. It is here changed to work with the Mountain Car environment.\n",
    "\n",
    "The original implementation implements the DQN algorithm with **fixed Q targets**, on the basis of image recognition from the screen.\n",
    "Here I present the following versions:\n",
    "1. Original **Fixed Q targets**, vision based DQN\n",
    "2. **Double DQN**, vision based model\n",
    "3. **Modified Reward to less sparse** – Built in a flag that modifies the reward to also reflect _in each step_ the distance from the goal, and reward the agent for any progress towards the goal, while still having a larger (\"jackpot\") reward at the goal. This allows for much faster learning since otherwise the agent needed to try random actions thousands of times at least before randomly reaching the goal.\n",
    "4. **DQN_Light** – non-vision based lighter version, which runs much faster, with the modified reward.\n",
    "\n",
    "\n",
    "\n",
    "**Task**\n",
    "The agent's goal is to get the car from the valley up to the goal post on the right. However, the engine of the car is not strong enough to climb that hill. It needs to learn eventually to use the back hill in order to get momentum and, like a swing, eventually gain enough momentum + engine power to reach the goal.\n",
    "Official leaderboard with various algorithms and visualizations at the\n",
    "`Gym website <https://gym.openai.com/envs/MountainCar-v0/>`__.\n",
    "\n",
    "\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action. \n",
    "\n",
    "**Environment Observation**: The environment observation consists of position and velocity of the car alone.\n",
    "\n",
    "**Actions**: The agent has to decide between three actions: moving the cart left, right, or do nothing.\n",
    "\n",
    "**Reward**: -1 for each time step, until the goal position of 0.5 is reached. As with MountainCarContinuous v0, there is no penalty for climbing the left hill, which upon reached acts as a wall.\n",
    "\n",
    "**Starting State**: Random position from -0.6 to -0.4 with no velocity.\n",
    "\n",
    "**Episode Ends**: The episode ends when you reach 0.5 position, or if 200 iterations are reached.\n",
    "\n",
    "While in the original tutorial the state was calculated as the `difference between the current screen patch and the previous one` - that was in order to allow the agent to take the velocity of the pole into account from one image. However, here we observe the velocity directly; so we don't need the difference, but can take directly the new state.\n",
    "\n",
    "\n",
    "**Packages**\n",
    "\n",
    "\n",
    "First, let's import needed packages. Firstly, we need\n",
    "`gym <https://gym.openai.com/docs>`__ for the environment\n",
    "(Install using `pip install gym`).\n",
    "We'll also use the following from PyTorch:\n",
    "\n",
    "-  neural networks (``torch.nn``)\n",
    "-  optimization (``torch.optim``)\n",
    "-  automatic differentiation (``torch.autograd``)\n",
    "-  utilities for vision tasks (``torchvision`` - `a separate\n",
    "   package <https://github.com/pytorch/vision>`__).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from time import time \n",
    "\n",
    "import ipympl\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import glob, os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make('MountainCar-v0').unwrapped\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(100)\n",
    "torch.manual_seed(100)\n",
    "env.seed(100)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training.\n",
    "\n",
    "**A future extension for this could be priorities experience replay. That would be implemented by saving the resulted penalty with each experience stored in memory (below in the `push` function) and then modifying the `sample` function to sample considering the \"badness\" of the experience**.\n",
    "\n",
    "However, there are more subtleties to this. In PER we ensure not only the same experiences are sampled all the time by applying stochastic normalization with a constant alpha. When performing gradient updates we weight experiences by their inverse priority (sampling probability) to reduce their impact on the weight update, this becomes more important when we get towards convergence. While these techniques seem like they would reduce the impact of prioritization slightly they improve and speed-up learning by ensuring difficult cases are seen appropriately often but don't overwhelm the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named tuple of transition and arg-names, for later inserting as memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    ''' \n",
    "    Experiences storage object which saves past experiences\n",
    "    and allows us to easily sample from them, \n",
    "    while capping the maximum capacity and observing the memory size.\n",
    "    \n",
    "    Arguments: \n",
    "        – Capacity: maximum number of experiences to store\n",
    "    Usage:\n",
    "        ReplayMemory.push() : save an experience into the object\n",
    "        ReplayMemory.sample() sample an experience randomly\n",
    "        len(ReplayMemory) – returns length of memory (amount of experiences stored)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\" Saves a transition. \"\"\"\n",
    "        # if we haven't reached maximum capacity, add another memory. \n",
    "        # otherwise, overwride an existing memory\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None) # add another memory slot\n",
    "        # save the last transition in the current [i] position in memory\n",
    "        self.memory[self.position] = Transition(*args) \n",
    "        # advance [i] position slot (or restart if reached capacity)\n",
    "        self.position = (self.position + 1) % self.capacity \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Samples an experience from memory randomly\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns length of memory \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DQN algorithm\n",
    "\n",
    "**The following paragraph is from the original PyTorch Tutorial, and still relevant to this implementation.**\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is also known as the *return*. The discount,\n",
    "$\\gamma$, should be a constant between $0$ and $1$\n",
    "that ensures the sum converges. It makes rewards from the uncertain far\n",
    "future less important for our agent than the ones in the near future\n",
    "that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
    "\n",
    "To minimise this error, we will use the `Huber\n",
    "loss <https://en.wikipedia.org/wiki/Huber_loss>`__. The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy. \n",
    "\n",
    "\n",
    "We calculate\n",
    "this over a batch of transitions, $B$, sampled from the replay\n",
    "memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}\n",
    "\n",
    "\n",
    "## DQN-1: Deep Convolutional Q-Network\n",
    "\n",
    "Our model will be a convolutional neural network that takes in the\n",
    "difference between the current and previous screen patches as its observation. \n",
    "The network is trying to predict the *quality* of\n",
    "taking each action given the current input.\n",
    "\n",
    "**This network structure will be used for both the policy network and the target networks, used separately to calculate the Q values and choose from them in Dual DQN**. \n",
    "\n",
    "This would be approximating the Qs of state and action pairs at time t:\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big[r_t + \\gamma \\max\\limits_{a} Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\big]  \\tag{Q-Learning}$$\n",
    "\n",
    "**The original implementation's inputs were Screen Captures. This is it below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Neural Network Architecture for DQN\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    ''' Vision based DQN: learn based on screen difference inputs'''\n",
    "\n",
    "    def __init__(self, net_type=\"fixed\"):\n",
    "        ''' Define DQN Network Class Architecture'''\n",
    "        # store type of network upon creation for later reference\n",
    "        self.net_type = net_type \n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # L1: 2D convolution, halving the size (stride=2), + batch norm\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        # L2: 2D convolution, halving the size (stride=2), + batch norm\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        # L3: 2D convolution, halving the size (stride=2), + batch norm\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        # L4: Last Fully Connected Linear layer defines action to take (left/right)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward propagate x input through the nextwork'''\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # x after L1\n",
    "        x = F.relu(self.bn2(self.conv2(x))) # x after L2\n",
    "        x = F.relu(self.bn3(self.conv3(x))) # x after L3\n",
    "        return self.head(x.view(x.size(0), -1)) # x after L4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN-2: \"Light\"  Neural Network for sparse input format\n",
    "##### direct velocity and position information\n",
    "\n",
    "Alternatively, below I add another option of how simple would the architechture be if we would use the version with direct information as inputs.\n",
    "\n",
    "In the MountainCar Problem: we have only a 1D input of length 2 (x_position, velocity).\n",
    "The action space we want to map to is one of 3 actions (left, nothing, right)\n",
    " Therefore we just need a flexible network to learn mapping between those inputs (size [1,2]) to value functions of the 3 possible outputs (output size: [1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_light(nn.Module):\n",
    "    ''' \n",
    "    Neural Network choosing action based on direct state value observations\n",
    "    Input: 1D of length 2 (x_position, velocity) \n",
    "    Output: 1D of length 3 (left, nothing, right)\n",
    "    '''    \n",
    "    def __init__(self, hidden = 64):\n",
    "        ''' Define DQN Network Class Architecture'''\n",
    "        self.net_type = \"light\"\n",
    "        super(DQN_light, self).__init__()\n",
    "        \n",
    "        # input is the size of the direct observation the environment gives us (1D array of 2x1)\n",
    "        self.input_size = env.observation_space.shape[0]\n",
    "        # output size is the Q for each possible action\n",
    "        self.output_size = env.action_space.n\n",
    "        # manually define number of hidden nodes \n",
    "        self.hidden_size = hidden \n",
    "        # 2 Fully connected layers with one hidden layer:\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size, bias=False) # L1, Linear/fully connected\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.output_size, bias=False) # L2, Linear/fully connected\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        '''Forward propagate x input through the nextwork'''\n",
    "        # propate x via fc1 then fc2 and return the neural net's ouptut\n",
    "        out_ff_net = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.fc2,\n",
    "        )\n",
    "        return out_ff_net(x)\n",
    "    \n",
    "def weights_init(m):\n",
    "    ''' initializes weights to all NN weight needed layers (linear FC layers here)'''\n",
    "    classname = m.__class__.__name__\n",
    "    # find all layers with \"Linear\" in their name (all our NN layers)\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen Input extraction\n",
    "\n",
    "The code below are utilities for extracting and processing rendered\n",
    "images from the environment. It uses the ``torchvision`` package, which\n",
    "makes it easy to compose image transforms. Once you run the cell it will\n",
    "display an example patch that it extracted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import torchvision.transforms as T\n",
    "# Resize image and turn into tensor\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "screen_width = 600\n",
    "\n",
    "def get_cart_location():\n",
    "    ''' Get the position of the cart (car/cart...)'''\n",
    "    world_width = env.max_position * 2 #modified to fit env\n",
    "    # if env==\"light\":\n",
    "    #    world_width = env.max_position\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    ''' Read the screen from input'''\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    \n",
    "    # Strip off the top and bottom of the screen, slice based on cart location\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "# uncomment the line below to see picture\n",
    "# plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),interpolation='none')\n",
    "# plt.title('Example extracted screen')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "----------------\n",
    "\n",
    "## Hyperparameters and Action Selection\n",
    "This cell instantiates our model and its optimizer, and the action selection function.\n",
    "\n",
    "-  ``select_action`` - will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "  \n",
    "Meaning, we start with mostly exploring (high epsilon = high probability of random action exploration), and with time the espilon decays, which makes as \"exploit\" more – take the maximal valued function from our Q estimates.\n",
    "\n",
    "##### Here we initialize the policy and target networks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# For Fixed Targets DQNs:\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "\n",
    "# ALTERNATIVELY; if you wanted the small and efficient network with \n",
    "## direct input information rather than vision oriented, \n",
    "# comment the above 2 lines and uncomment these below\n",
    "\n",
    "# policy_net = DQN_light().to(device)\n",
    "# policy_net.apply(weights_init)\n",
    "# target_net = DQN_light().to(device)\n",
    "\n",
    "# Copy parameters from PolicyNet to TargetNet\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state, net_type=0):\n",
    "    ''' function which selects best action: left/right based on Q network state values'''\n",
    "    global steps_done    \n",
    "    \n",
    "    # Threshold defines with which probability should we move left/right\n",
    "    # This is an epsilon-greedy exploration/exploitation strategy; where we exploit \n",
    "    # our Q knowledge but at each point we randomly explore with probability epsilon.\n",
    "    # To prioritize exploration at first but later on exploitation, \n",
    "    # we start with a large epsilon (probability of exploring random action) \n",
    "    # and decay it the epsilon threshold with time (steps)\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    # random draw for epsilon-greedy method - decide to randomly explore rather then exploit\n",
    "    sample = random.random()\n",
    "    \n",
    "    # Exploitation: If random>threshold, exploit our network\n",
    "    if sample > eps_threshold:        \n",
    "        # for light DQN:\n",
    "        if policy_net.net_type==\"light\":\n",
    "\n",
    "            # exploit: compute Q values for each action\n",
    "            Q = policy_net(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
    "            # with torch.no_grad(): #debug\n",
    "            _, action = torch.max(Q, -1) # choose action with max Q value\n",
    "            print(\"light DQN action = \", action.item()) #debug\n",
    "        \n",
    "        # for full DQN (either fixed targets or dual:)\n",
    "        else: \n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state).max(1)[1].view(1, 1)\n",
    "                # print(\"full DQN action = \", action.item()) #debug\n",
    "    \n",
    "    # Exploration: choose a random action between 0,1,2 (is random sample is within espilon)\n",
    "    else:        \n",
    "        action = torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long) \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: Optimization\n",
    "\n",
    "Finally, the code for training our model.\n",
    "\n",
    "Here, you can find an ``optimize_model`` function that performs a\n",
    "single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
    "loss. By defition we set $V(s) = 0$ if $s$ is a terminal\n",
    "state. We also use a target network to compute $V(s_{t+1})$ for\n",
    "added stability. The target network has its weights kept frozen most of\n",
    "the time, but is updated with the policy network's weights every so often.\n",
    "This is usually a set number of steps but we shall use episodes for\n",
    "simplicity.\n",
    "\n",
    "\n",
    "To minimise this error, we will use the `Huber\n",
    "loss <https://en.wikipedia.org/wiki/Huber_loss>`__. The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(method=\"fixed\"):\n",
    "    '''\n",
    "    Model Optimizer and Learning\n",
    "    Input: method\n",
    "    if Method==\"fixed\":\n",
    "        This will train a Fixed-Target DQN.\n",
    "    If method==\"double\":\n",
    "        This will instead train a double DQN.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # memory must be at least as large as BATCH_SIZE\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #if incorrect batch size, stop.\n",
    "        #print(\"Memory of memory replay must be at least Batch Size. It's currently smaller\")\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    ### Build the computation graph for Q-learning ###\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE)) # this would be for the DQN_Small\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)#TODO: #.cpu()\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Double DQN is a different way to calculate the Q Values for the next state:\n",
    "    # using a different network (target network) for that, rather than the same.\n",
    "    # here we implement this:\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    \n",
    "    ### (1) For a Fixed-Q-Target DQN:\n",
    "    if method==\"fixed\":\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    ### (2) For Double DQN instead: ###\n",
    "    elif method==\"double\":\n",
    "        _, best_next_action = policy_net(non_final_next_state).max(1, keep_dim=True) #location of max val in tensor\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states)[:, best_next_action]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clamps the gradients to (-1,1) in-place \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    # Update variables from back-prop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss # return loss for documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Performance Evaluation: Original Plotting durations function\n",
    "    \n",
    "# Performance Results Plotting function\n",
    "def plot_history(array, name):\n",
    "    \"quickly plot a history of performance indicators\"\n",
    "    plt.figure(2, figsize=[10,5])\n",
    "    p = pd.Series(array)\n",
    "    plt.plot(p, alpha=0.8)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(name)\n",
    "    plt.title(name + ' History in Training...')\n",
    "    return plt.show()\n",
    "\n",
    "def plot_final_position_with_ma(end_position_history):\n",
    "    ## plotting the final position at the end of each episode\n",
    "    plt.figure(2, figsize=[10,5])\n",
    "    p = pd.Series(end_position_history)\n",
    "    ma = p.rolling(10).mean()\n",
    "    plt.plot(p, alpha=0.8)\n",
    "    plt.plot(ma)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title('Car Final Position')\n",
    "    plt.savefig('Final Position - Modified.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1: Vision Based DQN – Fixed Targets, Dual DQN, Sparse and Timely Rewards\n",
    "\n",
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and initialize the ``state`` Tensor. Then, we sample\n",
    "an action, execute it, observe the next screen and the reward (always\n",
    "1), and optimize our model once. When the episode ends (our model\n",
    "fails), we restart the loop.\n",
    "\n",
    "#### Notice that now we have flags that allow us to modify the network between Fixed Targets, Dual DQN, Sparse and Timely Rewards.\n",
    "\n",
    "### 1. Fixed Q Targets\n",
    "This is the original version here. Fixed Q targets uses a separate network with a fixed parameter for estimating the TD target. The separate network is basically a copy of the first network, but its parameters are freezed and only update every N (say, a 1000) iterations. \n",
    "This method is because when we use the same network and same parameters (weights) for estimating the target and the Q value, there is a big correlation between the TD target and the parameters we are changing.\n",
    "So, as we update our weights, also are targets are moving. Think of this as like chasing a moving target. This may cause wild oscilations, since as we move towards the target, the target changes, and may change directions as well. This of it as chasing a chicken. (This wonderful analogy from this explanation here: https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)\n",
    "\n",
    "### 2. Double (/Dual) DQN\n",
    "Double DQN helps us reduce the overestimation of q values and, as a consequence, helps us train faster and have more stable learning.\n",
    "This is because, especially in the beginning or depending on the states we explored, we don't know everything yet, and thus our estimation owuld be biased. We know that the accuracy of q values depends on what action we tried and what neighboring states we explored. If non-optimal actions are regularly evaluated more highly, we will not exploit the best actions but think that we are, and thus never actually reach the goal.\n",
    "Doulbe DQN solves this by computing the Q target by two networks to decouple the action selection from the target Q value generation. \n",
    "\n",
    "It shows like this:\n",
    "1. The policy network selects the best action to take for the next state (the action with the highest Q value).\n",
    "2. The target network calculates the target Q value of taking that (already selected) action at the next state.\n",
    "\n",
    "### 3. Sparse and Timely Rewards\n",
    "The learning took too long, and I understood that it wasn't learning anything good because it was never recieving the reward! (or taking thousands of games to randomdly reach that goal). That is the problem with sparse rewards: the default reward in this envrionemnt is constant until the car reaches the goal where it changes to +1. Therefore, the agent doesn't know where to go most of the time. \n",
    "Hence, I created an option to modify the reward to be more timely (as we know, from the science of learning, that timely and action-based rewards teach the best!). That reward takes adds the **position of the car**, since it starts around 0 and needs to reach 0.5 (and the screen cuts off at 0.6, so it really just needs to aspire to the positive side of the X axis). It still has the original reward built in, such that when it reaches the goal it gets a \"jackpot\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop for vision based DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training Loop for vision based DQN\n",
    "\n",
    "def train_DQN(num_episodes = 5, method = \"fixed\", max_steps=200, adjust_reward=False):\n",
    "    ''' Training loop'''\n",
    "    start_time = time()\n",
    "    loss_history = []\n",
    "    reward_history = []\n",
    "    durations_history = []\n",
    "    end_position_history = []\n",
    "    successes = 0\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # show env play every 100 episodes\n",
    "        if i_episode % 100 == 0: env.render()\n",
    "        \n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        episode_loss = 0    # initialize loss counter for entire episode\n",
    "        episode_reward = 0  # initialize reward counter for entire episode\n",
    "\n",
    "        # Build state of current as difference of two frames\n",
    "        last_screen = get_screen()\n",
    "        current_screen = get_screen()\n",
    "        state = current_screen - last_screen\n",
    "\n",
    "        for t in count():\n",
    "            # Select and perform an action \n",
    "            # select action with dacying-epsilon-greedy strategy\n",
    "            action = select_action(state)\n",
    "            # perform action and save output of next step\n",
    "            observation, reward, done, _ = env.step(action.item())\n",
    "            \n",
    "            # MODIFIED REWARD \n",
    "            # The original reward only changes when agent reaches the goal; \n",
    "            # but that takes forever because of the steepness of the hill (sparse rewards).\n",
    "            # we can make things more obvious and faster for learning if we give some more\n",
    "            # information: reward for getting closer to the goal\n",
    "            # observation[0] is position; we want it to go more right to the position of 0.5\n",
    "            # ; approach a reward of 1 as it approaches position 0.5\n",
    "            reward = 0.5*reward + observation[0]   \n",
    "            reward = torch.tensor([reward], device=device) # save reward\n",
    "            \n",
    "            # Observe new state\n",
    "            last_screen = current_screen\n",
    "            current_screen = get_screen()\n",
    "            if not done:\n",
    "                # next state is the difference between screens, \n",
    "                # as a proxy of movemnet (which is a proxy for position and velocity)\n",
    "                next_state = current_screen - last_screen\n",
    "            else:\n",
    "                # if we're done, no next state\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            # for later pulling for memory replay\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            # HERE \"method\" SPECIFIES IF YOU WANT FIXED OR DUAL DQNs\n",
    "            loss = optimize_model(method)\n",
    "            \n",
    "            # Log performance metrics – add to sum up as episode counter\n",
    "            if loss: episode_loss += loss.item()\n",
    "            episode_reward += reward.item()\n",
    "            print(\"{}-{}: action = {}, reward = {}, loss={}, done?{}\".format(i_episode, t, action.item(), reward.item(), loss, done))\n",
    "            \n",
    "            # if game over (reached goal or reach max timesteps), record it\n",
    "            if done or (t > max_steps):\n",
    "                episode_durations.append(t + 1)\n",
    "                plot_durations()\n",
    "                break\n",
    "        \n",
    "        # Print Episode details\n",
    "        print(\"===Episode {}. Duration={}. Success={}. Loss={}. Reward={}===\".format(i_episode, t+1, done, episode_loss, episode_reward))\n",
    "        \n",
    "        # RECORD HISTORY (#modified for performance evaluation)\n",
    "        durations_history.append(t+1)\n",
    "        reward_history.append(reward.item())\n",
    "        loss_history.append(episode_loss)\n",
    "        \n",
    "        # Update the target network\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "    end_time = time()\n",
    "    env.render()\n",
    "    env.close()\n",
    "    plt.ioff()\n",
    "    print('{} – Completed all {} episodes in {} minutes'.format(end_time, num_episodes, (end_time-start_time)/60))\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Vision Based DQNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Fixed Targets DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Fixed Targets DQNs:\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "# Copy parameters from PolicyNet to TargetNet\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "print(policy_net.net_type)\n",
    "# TRAIN THE MODEL!\n",
    "train_DQN(num_episodes=300, method=\"fixed\", max_steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(loss_history, \"Loss\")\n",
    "plot_history(durations_history, \"Episode Duration\")\n",
    "plot_history(reward_history, \"Reward given\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Double (Dual) DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE MODEL!\n",
    "train_DQN(num_episodes=100, method=\"dual\", max_steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(loss_history, \"Loss\")\n",
    "plot_history(durations_history, \"Episode Duration\")\n",
    "plot_history(reward_history, \"Reward given\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** these took too long and didn't finish, then didn't work on google colab because of vision engine needed, then here completed a small number of espisodes. but somehow histories weren't saved_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 2: Light DQN with timely rewards\n",
    "\n",
    "This version and its policy evaluation is built upon: https://medium.com/@ts1829/solving-mountain-car-with-q-learning-b77bf71b1de2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative training version for Small / Sparse DQN version\n",
    "\n",
    "def train_lightDQN(num_episodes = 100, max_steps=200):\n",
    "    ''' Training loop'''\n",
    "    start_time = time()\n",
    "    loss_history = []\n",
    "    reward_history = []\n",
    "    durations_history = []\n",
    "    end_position_history = []\n",
    "    successes_history = []\n",
    "    print(\"{} – Starting training for {} episodes.\".format(start_time, num_episodes))\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "         # show env play every 100 episodes\n",
    "        if i_episode % 100 == 0: env.render() \n",
    "        \n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        episode_loss = 0    # initialize loss counter for entire episode\n",
    "        episode_reward = 0  # initialize reward counter for entire episode\n",
    "\n",
    "        # Build state of current as difference of two frames\n",
    "        state = env.state\n",
    "\n",
    "        for t in count():\n",
    "            # Select and perform an action \n",
    "            action = select_action(state)\n",
    "            # perform the selected action and observe reward and effect on environment\n",
    "            state2 , reward, done, _ = env.step(action.item())\n",
    "            \n",
    "            # next state is an array (\"box\") describing [position, velocity]\n",
    "            [position, velocity] = state2\n",
    "            \n",
    "            # MODIFY REWARD:\n",
    "            # Since the environment's reward is binary (1 for success at 0.5 and 0 otherwise),\n",
    "            # the agent has no information of how close is it and will take a long while to \n",
    "            # figure out where to go and what to do.\n",
    "            # Since the goal is to reach position (0.5), here I will change the reward to reflect that:\n",
    "            # reward is -1 until reaching goal, so adding 0.5 will be 0 until reaching goal\n",
    "            # so until reaching goal, reward = position; and reward==2 when reaching goal\n",
    "            # reward = torch.tensor([reward], device=device)            \n",
    "            reward = 0.5*reward + 0.5 + position \n",
    "\n",
    "            # Observe new state\n",
    "            \n",
    "            # last_screen = current_screen\n",
    "            # current_screen = get_screen()\n",
    "            \n",
    "            \n",
    "            # Update state, unless game is over (then finish)\n",
    "            if not done:\n",
    "                next_state = state2\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            # update model parameter using information from new state\n",
    "            loss = optimize_model()\n",
    "            \n",
    "            ## Done with step, now log and finish or repeat next step\n",
    "            \n",
    "            # Log performance metrics – add to sum up as episode counter\n",
    "            if loss: episode_loss += loss.item()\n",
    "            episode_reward += reward\n",
    "            success = done \n",
    "            \n",
    "            # if we succeeded / game over:\n",
    "            if done or (t > max_steps):\n",
    "                episode_durations.append(t + 1)\n",
    "                plot_durations()\n",
    "                break\n",
    "        \n",
    "        # Print Episode details\n",
    "        #print(\"Episode {} done. Success={}. End_pos={}. Duration={}. Loss={}. Reward={}\".format( (i_episode, success, state2, t+1, episode_loss, episode_reward)))\n",
    "        print(\"===Episode {}. Duration={}.  End_pos={}. Success={}. Loss={}. Reward={}===\".format(i_episode, t+1, state2[0], done, episode_loss, episode_reward))\n",
    "\n",
    "        # RECORD HISTORY (#modified for performance evaluation)\n",
    "        durations_history.append(t+1)\n",
    "        reward_history.append(reward)\n",
    "        loss_history.append(episode_loss)\n",
    "        end_position_history.append(state2[0])\n",
    "        successes_history.append(done)\n",
    "        # Update the target network\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    end_time = time()\n",
    "    env.close()\n",
    "    plt.ioff()\n",
    "    print('Completed all {} episodes, {} successfully - {}%'.format(num_episodes, np.sum(successes_history), np.sum(successes_history)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:37<00:00, 15.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed all 1000 episodes, 591 successfully - 59.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "# Prepare networks\n",
    "policy_net = DQN_light().to(device)\n",
    "policy_net.apply(weights_init)\n",
    "target_net = DQN_light().to(device)\n",
    "# Copy parameters from PolicyNet to TargetNet\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Train networks\n",
    "train_lightDQN(num_episodes=1000, max_steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light DQN Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1e82e58d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_final_position_with_ma(end_position_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After ~320 training episodes, our agent learned to reach the goal and starts reaching the goal often, but still exploring with some unsuccessful episodes interlaced. Before 400 episodes the agent moves to complete successfully almost all episodes, but still with some occasional unsuccessful explorations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"900\" position=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~ts1829/22.embed\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
